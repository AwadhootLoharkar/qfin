{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import quandl\n",
    "\n",
    "# Define start and end dates\n",
    "start = pd.to_datetime('2013-01-01')\n",
    "end = pd.to_datetime('2018-01-01')\n",
    "\n",
    "# Define ticker symbols in a list\n",
    "tickers = ['JPM.11', 'C.11', 'BAC.11', 'WFC.11']\n",
    "\n",
    "# Loop through tickers and download/save data\n",
    "for ticker in tickers:\n",
    "  # Download data for current ticker\n",
    "  data = quandl.get(f'WIKI/{ticker}', start_date=start, end_date=end)\n",
    "  \n",
    "  # Extract filename based on ticker symbol\n",
    "  filename = f\"{ticker.split('.')[0]}_CLOSE\"  # Remove extension from ticker\n",
    "  \n",
    "  # Save data to CSV\n",
    "  data.to_csv(filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a portfolio of stocks\n",
    "The first step is to import the requisite stocks data we’re going to be working with from quandl. We will work with a bunch of banking stocks namely, JP Morgan, Citi, Bank of America and Wells Fargo. Let’s take 5 years of adjusted close prices from 2013 to 2018 to create a stock portfolio and do our analysis. So, what we have here is five years worth of time series data from 2013 to 2018 in a simple dataframe for each of the four stocks. Each dataframe contains a date column as index and a column for adjusted close price."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizing Prices\n",
    "Next, we will normalize the prices by taking the adjusted close price at a particular date and dividing it by the very initial adj. close price. The normalized prices are stored in a new column called ‘Normed Return’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for stock_df in (jpm,citi,bofa,wfc):\n",
    "    stock_df['Normed Return'] = stock_df['Adj.Close']/stock_df.iloc[0]['Adj. Close']\n",
    "jpm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Allocations\n",
    "Next, we’ll assign an array of random weights for the purpose of calculation. These weights will represent the percentage allocation of investments between these four stocks. They must add up to 1.\n",
    "\n",
    "Let’s assume we had the following allocations for our total portfolio:\n",
    "\n",
    "30% in JP Morgan Chase\n",
    "20% in Citi\n",
    "40% in Bank of America\n",
    "10% in Wells Fargo\n",
    "Let’s have these values be reflected by multiplying our Normalized Returns by our allocations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for stock_df,allo in zip([jpm,citi,bofa,wfc],[.3,.2,.4,.1]): #Hard coding here\n",
    "    \n",
    "    stock_df['Allocation'] = stock_df['Normed Return']*allo\n",
    "jpm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Total Portfolio Value\n",
    "So now we can get a better idea of how the returns are portfolio wise. Let’s assume we invested a million dollars in this portfolio.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for stock_df in [jpm,citi,bofa,wfc]:\n",
    "    stock_df['Position Values'] = stock_df['Allocation']*1000000\n",
    "jpm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, (0.3 x 1 million dollars) is allocated in JP Morgan on day 1. So, this becomes our Position Value for JPM on day 1. At the very next day, since JPM went down the Position Value has now turned to a little less.\n",
    "\n",
    "We can expand this idea to other stocks and create a larger dataframe containing position values of each stock and also the total portfolio value at the end of each day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_val = pd.concat([jpm['Position Values'],citi['Position Values'],bofa['Position Values'],wfc['Position Values']],axis=1)\n",
    "portfolio_val.columns = ['JPM Pos','CITI Pos','BOFA Pos','WFC Pos']\n",
    "portfolio_val['Total Pos'] = portfolio_val.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, now we can see how our position values are changing on a day-to-day basis.\n",
    "\n",
    "We can also plot the total portfolio values and individual position values on a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "portfolio_val['Total Pos'].plot(figsize=(10,8))\n",
    "plt.title('Total Portfolio Value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_val.drop('Total Pos',axis=1).plot(kind='line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_val['Daily Return'] = portfolio_val['Total Pos'].pct_change(1)\n",
    "portfolio_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_ret = 100 * (portfolio_val['Total Pos'][-1]/portfolio_val['Total Pos'][0] -1 )\n",
    "print('Our cumulative return is {} percent!'.format(cum_ret))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_val['Daily Return'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_val['Daily Return'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SR = portfolio_val['Daily Return'].mean()/portfolio_val['Daily Return'].std()\n",
    "SR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASR = (252**0.5)*SR\n",
    "ASR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and get Daily Returns\n",
    "jpm = pd.read_csv('JPM_CLOSE',index_col='Date',parse_dates=True)\n",
    "citi = pd.read_csv('CITI_CLOSE',index_col='Date',parse_dates=True)\n",
    "bofa = pd.read_csv('BOFA_CLOSE',index_col='Date',parse_dates=True)\n",
    "wfc = pd.read_csv('WFC_CLOSE',index_col='Date',parse_dates=True)\n",
    "stocks = pd.concat([jpm,citi,bofa,wfc],axis=1)\n",
    "stocks.columns = ['jpm','citi','bofa','wfc']\n",
    "stocks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_ret = np.log(stocks/stocks.shift(1))\n",
    "log_ret.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the log return mean of each stock\n",
    "log_ret.mean() * 252"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute pairwise covariance of columns\n",
    "log_ret.cov()*252"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(101)\n",
    "\n",
    "# Stock Columns\n",
    "print('Stocks')\n",
    "print(stocks.columns)\n",
    "print('\\n')\n",
    "\n",
    "# Create Random Weights\n",
    "print('Creating Random Weights')\n",
    "weights = np.array(np.random.random(4))\n",
    "print(weights)\n",
    "print('\\n')\n",
    "\n",
    "# Rebalance Weights\n",
    "print('Rebalance to sum to 1.0')\n",
    "weights = weights / np.sum(weights)\n",
    "print(weights)\n",
    "print('\\n')\n",
    "\n",
    "# Expected Return\n",
    "print('Expected Portfolio Return')\n",
    "exp_ret = np.sum(log_ret.mean() * weights) *252\n",
    "print(exp_ret)\n",
    "print('\\n')\n",
    "\n",
    "# Expected Variance\n",
    "print('Expected Volatility')\n",
    "exp_vol = np.sqrt(np.dot(weights.T, np.dot(log_ret.cov() * 252, weights)))\n",
    "print(exp_vol)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_ports = 20000\n",
    "\n",
    "all_weights = np.zeros((num_ports,len(stocks.columns)))\n",
    "ret_arr = np.zeros(num_ports)\n",
    "vol_arr = np.zeros(num_ports)\n",
    "sharpe_arr = np.zeros(num_ports)\n",
    "\n",
    "for ind in range(num_ports):\n",
    "\n",
    "    # Create Random Weights\n",
    "    weights = np.array(np.random.random(4))\n",
    "\n",
    "    # Rebalance Weights\n",
    "    weights = weights / np.sum(weights)\n",
    "    \n",
    "    # Save Weights\n",
    "    all_weights[ind,:] = weights\n",
    "\n",
    "    # Expected Return\n",
    "    ret_arr[ind] = np.sum((log_ret.mean() * weights) *252)\n",
    "\n",
    "    # Expected Variance\n",
    "    vol_arr[ind] = np.sqrt(np.dot(weights.T, np.dot(log_ret.cov() * 252, weights)))\n",
    "\n",
    "    # Sharpe Ratio\n",
    "    sharpe_arr[ind] = ret_arr[ind]/vol_arr[ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sharpe_arr.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sharpe_arr.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_weights[1248,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sr_ret = ret_arr[1248]\n",
    "max_sr_vol = vol_arr[1248]\n",
    "plt.figure(figsize=(17,9))\n",
    "plt.scatter(vol_arr,ret_arr,c=sharpe_arr,cmap='plasma')\n",
    "plt.colorbar(label='Sharpe Ratio')\n",
    "plt.xlabel('Volatility')\n",
    "plt.ylabel('Return')\n",
    "# Add red dot for max SR\n",
    "plt.scatter(max_sr_vol,max_sr_ret,c='red',s=50,edgecolors='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ret_vol_sr(weights):\n",
    "    \"\"\"\n",
    "    Takes in weights and returns back an array of mean return, mean volatility and sharpe ratio\n",
    "    \"\"\"\n",
    "    weights = np.array(weights)\n",
    "    ret = np.sum(log_ret.mean() * weights) * 252\n",
    "    vol = np.sqrt(np.dot(weights.T, np.dot(log_ret.cov() * 252, weights)))\n",
    "    sr = ret/vol\n",
    "    return np.array([ret,vol,sr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg_sharpe(weights):\n",
    "    return  get_ret_vol_sr(weights)[2] * -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constraints\n",
    "def check_sum(weights):\n",
    "    '''\n",
    "    Returns 0 if sum of weights is 1.0\n",
    "    '''\n",
    "    return np.sum(weights) - 1\n",
    "# By convention of minimize function it should be a function that returns zero for conditions\n",
    "cons = ({'type':'eq','fun': check_sum})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0-1 bounds for each weight\n",
    "bounds = ((0, 1), (0, 1), (0, 1), (0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial Guess (equal distribution)\n",
    "init_guess = [0.25,0.25,0.25,0.25]\n",
    "# Sequential Least Squares Programming (SLSQP).\n",
    "opt_results = minimize(neg_sharpe,init_guess,method='SLSQP',bounds=bounds,constraints=cons)\n",
    "opt_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_results.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ret_vol_sr(opt_results.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our returns go from 0 to somewhere along 0.2\n",
    "# Create a linspace number of points to calculate x on\n",
    "frontier_y = np.linspace(0,0.2,100) \n",
    "def minimize_volatility(weights):\n",
    "    return  get_ret_vol_sr(weights)[1]\n",
    "frontier_volatility = []\n",
    "for possible_return in frontier_y:\n",
    "    # function for return\n",
    "    cons = ({'type':'eq','fun': check_sum},\n",
    "            {'type':'eq','fun': lambda w: get_ret_vol_sr(w)[0] - possible_return})\n",
    "    \n",
    "    result = minimize(minimize_volatility,init_guess,method='SLSQP',bounds=bounds,constraints=cons)\n",
    "    \n",
    "    frontier_volatility.append(result['fun'])\n",
    "plt.figure(figsize=(17,9))\n",
    "plt.scatter(vol_arr,ret_arr,c=sharpe_arr,cmap='plasma')\n",
    "plt.colorbar(label='Sharpe Ratio')\n",
    "plt.xlabel('Volatility')\n",
    "plt.ylabel('Return')\n",
    "# Add frontier line\n",
    "plt.plot(frontier_volatility,frontier_y,'g--',linewidth=3)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
